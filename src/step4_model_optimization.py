import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from torch.cuda import amp  # Mixed precision training
from sklearn.metrics import accuracy_score, classification_report, precision_recall_fscore_support
from sklearn.model_selection import KFold
import joblib
import time
import os
from sklearn.model_selection import ParameterGrid
import warnings
warnings.filterwarnings('ignore')

# Ki·ªÉm tra GPU - Y√äU C·∫¶U PH·∫¢I C√ì GPU
if not torch.cuda.is_available():
    print("‚ùå L·ªñI: Kh√¥ng t√¨m th·∫•y GPU CUDA! Script n√†y y√™u c·∫ßu GPU RTX 3080 Ti ƒë·ªÉ ch·∫°y.")
    print("Vui l√≤ng ki·ªÉm tra:")
    print("1. ƒê√£ c√†i ƒë·∫∑t CUDA toolkit")
    print("2. ƒê√£ c√†i ƒë·∫∑t PyTorch v·ªõi CUDA support")
    print("3. GPU driver ƒë√£ ƒë∆∞·ª£c c·∫≠p nh·∫≠t")
    exit(1)

device = torch.device('cuda')
print(f"‚úÖ S·ª≠ d·ª•ng GPU: {torch.cuda.get_device_name(0)}")
print(f"üìä GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")

# T·ªëi ∆∞u h√≥a GPU cho RTX 3080 Ti
torch.backends.cudnn.benchmark = True
torch.backends.cudnn.enabled = True
torch.backends.cuda.matmul.allow_tf32 = True
torch.backends.cudnn.allow_tf32 = True

print("üöÄ ƒê√£ k√≠ch ho·∫°t c√°c t·ªëi ∆∞u h√≥a GPU: cuDNN benchmark, TF32 precision")

# T·∫°o th∆∞ m·ª•c models n·∫øu ch∆∞a c√≥
os.makedirs('./models', exist_ok=True)
os.makedirs('./reports', exist_ok=True)

# Load d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω
print("Loading processed data...")
# Gi·∫£ ƒë·ªãnh c√°c file d·ªØ li·ªáu n·∫±m trong th∆∞ m·ª•c ./data/
# N·∫øu kh√¥ng, h√£y thay ƒë·ªïi ƒë∆∞·ªùng d·∫´n cho ph√π h·ª£p
try:
    X_train = np.load('./data/X_train.npy', allow_pickle=True)
    X_test = np.load('./data/X_test.npy', allow_pickle=True)
    y_train = np.load('./data/y_train.npy', allow_pickle=True)
    y_test = np.load('./data/y_test.npy', allow_pickle=True)
except FileNotFoundError as e:
    print(f"L·ªói: {e}. H√£y ch·∫Øc ch·∫Øn r·∫±ng c√°c file .npy t·ªìn t·∫°i trong th∆∞ m·ª•c ./data/")
    exit()


print(f"Training data shape: {X_train.shape}")
print(f"Test data shape: {X_test.shape}")
print(f"Number of classes: {len(np.unique(y_train))}")

# Load label encoder
le = joblib.load('./models/label_encoder.joblib')
y_train_encoded = le.transform(y_train)
y_test_encoded = le.transform(y_test)

# ==================== ƒê·ªãnh nghƒ©a m√¥ h√¨nh CNN v·ªõi tham s·ªë c√≥ th·ªÉ tune (ƒê√É S·ª¨A) ====================

class TunableCNN(nn.Module):
    # S·ª¨A 1: Th√™m `seq_length` ƒë·ªÉ m√¥ h√¨nh kh√¥ng ph·ª• thu·ªôc v√†o bi·∫øn to√†n c·ª•c
    def __init__(self, vocab_size, embed_dim, num_classes, seq_length, dropout_rate=0.5, num_filters=128):
        super(TunableCNN, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.conv1 = nn.Conv1d(embed_dim, num_filters, kernel_size=3, padding=1)
        self.conv2 = nn.Conv1d(num_filters, num_filters//2, kernel_size=3, padding=1)
        self.pool = nn.MaxPool1d(2)
        self.dropout = nn.Dropout(dropout_rate)

        # S·ª¨A 1: T√≠nh output size m·ªôt c√°ch t·ª± ƒë·ªông ƒë·ªÉ m√¥ h√¨nh linh ho·∫°t
        conv_output_size = self._get_conv_output_size(embed_dim, seq_length)
        
        self.fc1 = nn.Linear(conv_output_size, 128)
        self.fc2 = nn.Linear(128, num_classes)

    def _get_conv_output_size(self, embed_dim, seq_length):
        # T·∫°o m·ªôt tensor gi·∫£ ƒë·ªÉ t√≠nh to√°n k√≠ch th∆∞·ªõc ƒë·∫ßu ra sau c√°c l·ªõp conv/pool
        with torch.no_grad():
            dummy_input = torch.zeros(1, embed_dim, seq_length)
            x = self.pool(torch.relu(self.conv1(dummy_input)))
            x = self.pool(torch.relu(self.conv2(x)))
            return x.view(1, -1).size(1)

    def forward(self, x):
        x = self.embedding(x).permute(0, 2, 1)  # (batch, embed_dim, seq_len)
        x = torch.relu(self.conv1(x))
        x = self.pool(x)
        x = torch.relu(self.conv2(x))
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        x = self.dropout(x)
        x = torch.relu(self.fc1(x))
        x = self.dropout(x)
        x = self.fc2(x)
        return x

# ==================== H√†m training v·ªõi early stopping (ƒê√É S·ª¨A) ====================

def train_model_early_stopping(model, train_loader, val_loader, criterion, optimizer,
                              num_epochs=50, patience=5, min_delta=0.001):
    model.to(device)
    scaler = amp.GradScaler()  # Mixed precision scaler
    best_loss = float('inf')
    patience_counter = 0
    best_model_state = None

    for epoch in range(num_epochs):
        model.train()
        train_loss = 0
        for inputs, labels in train_loader:
            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)

            optimizer.zero_grad()

            # Mixed precision training
            with amp.autocast():
                outputs = model(inputs)
                loss = criterion(outputs, labels)

            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()

            train_loss += loss.item()
        train_loss /= len(train_loader)

        model.eval()
        val_loss = 0
        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)
                with amp.autocast():
                    outputs = model(inputs)
                    loss = criterion(outputs, labels)
                val_loss += loss.item()
        val_loss /= len(val_loader)

        # S·ª¨A 2: S·ª≠a l·ªánh print ƒë·ªÉ hi·ªÉn th·ªã th√¥ng tin h·ªØu √≠ch
        if (epoch + 1) % 5 == 0:
            print(f"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}")

        if val_loss < best_loss - min_delta:
            best_loss = val_loss
            patience_counter = 0
            best_model_state = model.state_dict().copy()
        else:
            patience_counter += 1

        if patience_counter >= patience:
            print(f"Early stopping at epoch {epoch+1}")
            break

    if best_model_state:
        model.load_state_dict(best_model_state)
    return model, best_loss

# ==================== H√†m ƒë√°nh gi√° m√¥ h√¨nh ====================

def evaluate_model(model, test_loader):
    model.eval()
    all_preds = []
    all_labels = []
    with torch.no_grad():
        for inputs, labels in test_loader:
            inputs, labels = inputs.to(device, non_blocking=True), labels.to(device, non_blocking=True)
            with amp.autocast():
                outputs = model(inputs)
                _, preds = torch.max(outputs, 1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
    return np.array(all_preds), np.array(all_labels)

# ==================== Cross-validation (ƒê√É S·ª¨A) ====================

# S·ª¨A 3: Truy·ªÅn `vocab_size` v√†o h√†m thay v√¨ t√≠nh to√°n l·∫°i m·ªói l·∫ßn
def cross_validate_model(model_class, params, X, y, vocab_size, n_splits=3):
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)
    cv_scores = []

    for fold, (train_idx, val_idx) in enumerate(kf.split(X)):
        print(f"Fold {fold+1}/{n_splits}")

        X_train_fold, X_val_fold = X[train_idx], X[val_idx]
        y_train_fold, y_val_fold = y[train_idx], y[val_idx]

        train_dataset = TensorDataset(torch.LongTensor(X_train_fold), torch.LongTensor(y_train_fold))
        val_dataset = TensorDataset(torch.LongTensor(X_val_fold), torch.LongTensor(y_val_fold))
        train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True, pin_memory=True)
        val_loader = DataLoader(val_dataset, batch_size=params['batch_size'], shuffle=False, pin_memory=True)

        # S·ª¨A 1 & 3: Kh·ªüi t·∫°o m√¥ h√¨nh v·ªõi seq_length v√† vocab_size ƒë√£ ƒë∆∞·ª£c t√≠nh to√°n ƒë√∫ng
        seq_length = X_train_fold.shape[1]
        model = model_class(vocab_size, params['embed_dim'], len(np.unique(y)), seq_length,
                           params['dropout_rate'], params['num_filters'])
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'], weight_decay=params['weight_decay'])

        trained_model, val_loss = train_model_early_stopping(
            model, train_loader, val_loader, criterion, optimizer,
            num_epochs=30, patience=3
        )

        cv_scores.append(val_loss)
        # S·ª¨A 2: S·ª≠a l·ªánh print
        print(f"Fold {fold+1} Validation Loss: {val_loss:.4f}")

    return np.mean(cv_scores), np.std(cv_scores)

# ==================== Hyperparameter tuning (ƒê√É S·ª¨A) ====================

# S·ª¨A 3: T√≠nh vocab_size m·ªôt l·∫ßn ·ªü ƒë√¢y
vocab_size = int(np.max(X_train)) + 1
num_classes = len(np.unique(y_train_encoded))
seq_length = X_train.shape[1]

print(f"Vocab size: {vocab_size}")
print(f"Number of classes: {num_classes}")
print(f"Sequence length: {seq_length}")

param_grid = {
    'embed_dim': [100, 150],
    'dropout_rate': [0.3, 0.5],
    'num_filters': [64, 128],
    'learning_rate': [0.001, 0.0005],
    'batch_size': [32, 64, 128],  # TƒÉng batch size ƒë·ªÉ t·ªëi ∆∞u GPU RTX 3080 Ti
    'weight_decay': [1e-4, 1e-5]
}

print(f"Total parameter combinations: {len(list(ParameterGrid(param_grid)))}")

if __name__ == '__main__':
    best_params = None
    best_score = float('inf')
    results = []

    print("\n" + "="*60)
    print("HYPERPARAMETER TUNING WITH CROSS-VALIDATION")
    print("="*60)

    for i, params in enumerate(ParameterGrid(param_grid)):
        print(f"\nTesting combination {i+1}/{len(list(ParameterGrid(param_grid)))}: {params}")
        try:
            # S·ª¨A 3: Truy·ªÅn vocab_size v√†o h√†m
            mean_score, std_score = cross_validate_model(TunableCNN, params, X_train, y_train_encoded, vocab_size, n_splits=3)
            results.append({
                'params': params,
                'mean_cv_score': mean_score,
                'std_cv_score': std_score
            })

            # S·ª¨A 2: S·ª≠a l·ªánh print
            print(f"Result: Mean CV Loss = {mean_score:.4f} (+/- {std_score:.4f})")

            if mean_score < best_score:
                best_score = mean_score
                best_params = params
                print("üéØ New best parameters found!")
        except Exception as e:
            print(f"Error with params {params}: {e}")
            continue

    if best_params is None:
        print("\nKh√¥ng t√¨m th·∫•y tham s·ªë t·ªët nh·∫•t. Vui l√≤ng ki·ªÉm tra l·∫°i qu√° tr√¨nh tuning.")
        exit()

    print(f"\nüèÜ Best parameters: {best_params}")
    # S·ª¨A 2: S·ª≠a l·ªánh print
    print(f"Best CV Loss: {best_score:.4f}")

    # ==================== Train final model v·ªõi best parameters (ƒê√É S·ª¨A) ====================
    print("\n" + "="*60)
    print("TRAINING FINAL OPTIMIZED MODEL")
    print("="*60)

    train_dataset = TensorDataset(torch.LongTensor(X_train), torch.LongTensor(y_train_encoded))
    test_dataset = TensorDataset(torch.LongTensor(X_test), torch.LongTensor(y_test_encoded))
    train_loader = DataLoader(train_dataset, batch_size=best_params['batch_size'], shuffle=True, pin_memory=True)
    test_loader = DataLoader(test_dataset, batch_size=best_params['batch_size'], shuffle=False, pin_memory=True)

    # S·ª¨A 1: Kh·ªüi t·∫°o m√¥ h√¨nh cu·ªëi c√πng v·ªõi seq_length
    final_model = TunableCNN(vocab_size, best_params['embed_dim'], num_classes, seq_length,
                            best_params['dropout_rate'], best_params['num_filters'])
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(final_model.parameters(), lr=best_params['learning_rate'], weight_decay=best_params['weight_decay'])
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)

    val_size = int(0.1 * len(train_dataset))
    train_size = len(train_dataset) - val_size
    train_subset, val_subset = torch.utils.data.random_split(train_dataset, [train_size, val_size])
    train_loader_final = DataLoader(train_subset, batch_size=best_params['batch_size'], shuffle=True, pin_memory=True)
    val_loader_final = DataLoader(val_subset, batch_size=best_params['batch_size'], shuffle=False, pin_memory=True)

    start_time = time.time()
    trained_final_model, final_val_loss = train_model_early_stopping(
        final_model, train_loader_final, val_loader_final, criterion, optimizer, num_epochs=50, patience=7
    )
    training_time = time.time() - start_time

    final_preds, final_labels = evaluate_model(trained_final_model, test_loader)
    final_accuracy = accuracy_score(final_labels, final_preds)
    final_precision, final_recall, final_f1, _ = precision_recall_fscore_support(final_labels, final_preds, average='weighted')

    print("\nüéâ FINAL MODEL RESULTS:")
    print(f"Accuracy: {final_accuracy:.4f}")
    print(f"Precision: {final_precision:.4f}")
    print(f"Recall: {final_recall:.4f}")
    print(f"F1-Score: {final_f1:.4f}")
    print(f"Training Time: {training_time:.2f} seconds")

    # ==================== So s√°nh v·ªõi m√¥ h√¨nh g·ªëc (ƒê√É S·ª¨A) ====================
    print("\n" + "="*60)
    print("COMPARISON WITH ORIGINAL MODEL")
    print("="*60)

    # S·ª¨A 4: X·ª≠ l√Ω tr∆∞·ªùng h·ª£p file m√¥ h√¨nh g·ªëc kh√¥ng t·ªìn t·∫°i
    try:
        # S·ª¨A 1: Kh·ªüi t·∫°o m√¥ h√¨nh g·ªëc v·ªõi seq_length
        original_model = TunableCNN(vocab_size, 100, num_classes, seq_length, 0.5, 128)
        original_model.load_state_dict(torch.load('./models/cnn_model.pth'))
        original_model.to(device)

        orig_preds, orig_labels = evaluate_model(original_model, test_loader)
        orig_accuracy = accuracy_score(orig_labels, orig_preds)
        orig_precision, orig_recall, orig_f1, _ = precision_recall_fscore_support(orig_labels, orig_preds, average='weighted')

    except FileNotFoundError:
        print("‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y file m√¥ h√¨nh g·ªëc './models/cnn_model.pth'. B·ªè qua b∆∞·ªõc so s√°nh.")
        orig_accuracy, orig_precision, orig_recall, orig_f1 = 0.0, 0.0, 0.0, 0.0

    print("üìä MODEL COMPARISON:")
    print(f"Original CNN:  Accuracy = {orig_accuracy:.4f}, Precision = {orig_precision:.4f}, Recall = {orig_recall:.4f}, F1 = {orig_f1:.4f}")
    print(f"Optimized CNN: Accuracy = {final_accuracy:.4f}, Precision = {final_precision:.4f}, Recall = {final_recall:.4f}, F1 = {final_f1:.4f}")
    # S·ª¨A 2: Lo·∫°i b·ªè l·ªánh print kh√¥ng c·∫ßn thi·∫øt

    # ==================== L∆∞u m√¥ h√¨nh v√† k·∫øt qu·∫£ ====================
    torch.save(trained_final_model.state_dict(), './models/cnn_optimized.pth')
    joblib.dump(best_params, './models/best_hyperparams.joblib')

    optimization_results = {
        'best_params': best_params,
        'best_cv_score': best_score,
        'final_accuracy': final_accuracy,
        'final_precision': final_precision,
        'final_recall': final_recall,
        'final_f1': final_f1,
        'training_time': training_time,
        'original_accuracy': orig_accuracy,
        'improvement': final_accuracy - orig_accuracy,
        'all_results': results,
        'timestamp': time.strftime("%Y-%m-%d %H:%M:%S")
    }
    joblib.dump(optimization_results, './models/step4_optimization_results.joblib')

    # ==================== T·∫°o b√°o c√°o HTML ====================
    html_content = f"""
<!DOCTYPE html>
<html lang="vi">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>B∆∞·ªõc 4: T·ªëi ∆Øu H√≥a M√¥ H√¨nh - B√°o C√°o</title>
    <style>
        body {{ font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; line-height: 1.6; margin: 0; padding: 20px; background-color: #f5f5f5; }}
        .container {{ max-width: 1200px; margin: 0 auto; background: white; padding: 30px; border-radius: 10px; box-shadow: 0 0 20px rgba(0,0,0,0.1); }}
        h1 {{ color: #2c3e50; text-align: center; margin-bottom: 30px; border-bottom: 3px solid #3498db; padding-bottom: 10px; }}
        h2 {{ color: #34495e; margin-top: 30px; border-left: 4px solid #3498db; padding-left: 15px; }}
        .comparison-table {{ display: grid; grid-template-columns: 1fr 1fr; gap: 20px; margin: 20px 0; }}
        .model-card {{ background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 20px; border-radius: 10px; text-align: center; box-shadow: 0 4px 15px rgba(0,0,0,0.2); }}
        .improvement {{ background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%); transform: scale(1.05); }}
        .metric-value {{ font-size: 2em; font-weight: bold; margin: 10px 0; }}
        .metric-label {{ font-size: 0.9em; opacity: 0.9; }}
        table {{ width: 100%; border-collapse: collapse; margin: 20px 0; background: white; }}
        th, td {{ padding: 12px; text-align: left; border-bottom: 1px solid #ddd; }}
        th {{ background-color: #3498db; color: white; }}
        tr:nth-child(even) {{ background-color: #f8f9fa; }}
        tr:hover {{ background-color: #e8f4fd; }}
        .code {{ background: #f8f9fa; padding: 15px; border-radius: 5px; border-left: 4px solid #3498db; margin: 10px 0; font-family: 'Courier New', monospace; }}
        .highlight {{ background: #fff3cd; padding: 15px; border-radius: 5px; border-left: 4px solid #ffc107; margin: 15px 0; }}
        .success {{ background: #d4edda; border-left-color: #28a745; }}
        .footer {{ text-align: center; margin-top: 40px; color: #7f8c8d; font-size: 0.9em; }}
    </style>
</head>
<body>
    <div class="container">
        <h1>üöÄ B∆∞·ªõc 4: T·ªëi ∆Øu H√≥a M√¥ H√¨nh - B√°o C√°o</h1>
        <div class="highlight">
            <strong>üéØ M·ª•c ti√™u:</strong> C·∫£i thi·ªán hi·ªáu su·∫•t m√¥ h√¨nh CNN t·ª´ {orig_accuracy:.1%} l√™n m·ª©c t·ªëi ∆∞u b·∫±ng hyperparameter tuning v√† cross-validation.
        </div>
        <h2>üèÜ K·∫øt Qu·∫£ T·ªëi ∆Øu H√≥a</h2>
        <div class="comparison-table">
            <div class="model-card">
                <div class="metric-label">M√¥ H√¨nh G·ªëc</div>
                <div class="metric-value">{orig_accuracy:.1%}</div>
                <div class="metric-label">Accuracy</div>
            </div>
            <div class="model-card improvement">
                <div class="metric-label">M√¥ H√¨nh T·ªëi ∆Øu</div>
                <div class="metric-value">{final_accuracy:.1%}</div>
                <div class="metric-label">Accuracy (+{((final_accuracy-orig_accuracy)*100):.1f}%)</div>
            </div>
        </div>
        <h2>‚öôÔ∏è Best Hyperparameters</h2>
        <div class="code">
"""
    for key, value in best_params.items():
        html_content += f"<strong>{key}:</strong> {value}<br>"
    html_content += f"""
        </div>
        <h2>üìä Chi Ti·∫øt Performance</h2>
        <table>
            <thead><tr><th>Metric</th><th>M√¥ H√¨nh G·ªëc</th><th>M√¥ H√¨nh T·ªëi ∆Øu</th><th>C·∫£i Thi·ªán</th></tr></thead>
            <tbody>
                <tr><td>Accuracy</td><td>{orig_accuracy:.4f}</td><td>{final_accuracy:.4f}</td><td>+{((final_accuracy-orig_accuracy)*100):.2f}%</td></tr>
                <tr><td>Precision</td><td>{orig_precision:.4f}</td><td>{final_precision:.4f}</td><td>+{((final_precision-orig_precision)*100):.2f}%</td></tr>
                <tr><td>Recall</td><td>{orig_recall:.4f}</td><td>{final_recall:.4f}</td><td>+{((final_recall-orig_recall)*100):.2f}%</td></tr>
                <tr><td>F1-Score</td><td>{orig_f1:.4f}</td><td>{final_f1:.4f}</td><td>+{((final_f1-orig_f1)*100):.2f}%</td></tr>
            </tbody>
        </table>
        <h2>üîç Ph√¢n T√≠ch K·∫øt Qu·∫£</h2>
        <div class="highlight success">
            <strong>‚ú® Th√†nh c√¥ng:</strong><br>
            - <strong>Accuracy c·∫£i thi·ªán</strong> t·ª´ {orig_accuracy:.1%} l√™n {final_accuracy:.1%} (+{((final_accuracy-orig_accuracy)*100):.1f}%)<br>
            - <strong>Cross-validation</strong> ƒë·∫£m b·∫£o robustness v·ªõi CV score: {best_score:.4f}<br>
            - <strong>Early stopping</strong> v√† <strong>learning rate scheduling</strong> t·ªëi ∆∞u h√≥a training<br>
            - <strong>Th·ªùi gian training</strong>: {training_time:.2f} gi√¢y v·ªõi GPU
        </div>
        <h2>üõ†Ô∏è K·ªπ Thu·∫≠t T·ªëi ∆Øu H√≥a √Åp D·ª•ng</h2>
        <ul>
            <li><strong>Grid Search</strong>: T√¨m ki·∫øm tr√™n {len(list(ParameterGrid(param_grid)))} combinations</li>
            <li><strong>3-Fold Cross-Validation</strong>: ƒê√°nh gi√° robust tr√™n nhi·ªÅu splits</li>
            <li><strong>Early Stopping</strong>: D·ª´ng khi validation loss kh√¥ng c·∫£i thi·ªán</li>
            <li><strong>Learning Rate Scheduling</strong>: Gi·∫£m LR khi plateau</li>
            <li><strong>Weight Decay (L2)</strong>: Regularization tr√°nh overfitting</li>
            <li><strong>Dropout Tuning</strong>: T·ªëi ∆∞u regularization rate</li>
        </ul>
        <div class="footer">
            <p>‚è∞ B√°o c√°o ƒë∆∞·ª£c t·∫°o v√†o: {time.strftime("%Y-%m-%d %H:%M:%S")}</p>
        </div>
    </div>
</body>
</html>
"""

    with open('./reports/Step4_Model_Optimization.html', 'w', encoding='utf-8') as f:
        f.write(html_content)

    print(f"\n‚úÖ B√°o c√°o HTML ƒë√£ ƒë∆∞·ª£c t·∫°o: ./reports/Step4_Model_Optimization.html")
    print("‚úÖ M√¥ h√¨nh t·ªëi ∆∞u ƒë√£ ƒë∆∞·ª£c l∆∞u: ./models/cnn_optimized.pth")
    print("‚úÖ Best hyperparameters: ./models/best_hyperparams.joblib")
    print("‚úÖ K·∫øt qu·∫£ optimization: ./models/step4_optimization_results.joblib")

    print("\nüéâ Ho√†n th√†nh B∆∞·ªõc 4: T·ªëi ∆∞u h√≥a m√¥ h√¨nh!")