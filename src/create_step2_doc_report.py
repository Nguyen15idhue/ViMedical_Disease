from docx import Document
from docx.shared import Inches, Pt
from docx.enum.text import WD_ALIGN_PARAGRAPH
from docx.enum.style import WD_STYLE_TYPE
import pandas as pd
import numpy as np
import joblib
from collections import Counter
from underthesea import word_tokenize
import time

# Load dá»¯ liá»‡u gá»‘c
train_data = pd.read_csv('data/train_data.csv')
test_data = pd.read_csv('data/test_data.csv')

# Táº¡o láº¡i cá»™t Processed_Question
def preprocess_text(text):
    tokens = word_tokenize(text.lower())
    stopwords = ['tÃ´i', 'Ä‘ang', 'lÃ ', 'cÃ³', 'khÃ´ng', 'vÃ ', 'hoáº·c', 'nhÆ°ng', 'náº¿u', 'thÃ¬']
    tokens = [word for word in tokens if word not in stopwords and word.isalpha()]
    return ' '.join(tokens)

train_data['Processed_Question'] = train_data['Question'].apply(preprocess_text)

# Load dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½
X_train_ml = np.load('data/X_train.npy')
X_test_ml = np.load('data/X_test.npy')
y_train = np.load('data/y_train.npy')
y_test = np.load('data/y_test.npy')

# Load deep learning data
X_train_dl = np.load('data/X_train_dl.npy')
X_test_dl = np.load('data/X_test_dl.npy')

# Load tokenizer vÃ  metadata
tokenizer = joblib.load('data/tokenizer.joblib')
label_encoder = joblib.load('data/label_encoder.joblib')
dl_metadata = joblib.load('data/dl_metadata.joblib')

# Táº¡o document
doc = Document()

# TiÃªu Ä‘á» chÃ­nh
title = doc.add_heading('ViMedical - Há»‡ ChuyÃªn Gia Bá»‡nh Táº­t', 0)
title.alignment = WD_ALIGN_PARAGRAPH.CENTER

subtitle = doc.add_heading('BÆ°á»›c 2: Tiá»n Xá»­ LÃ½ Dá»¯ Liá»‡u', 1)
subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER

# ThÃªm thÃ´ng tin tá»•ng quan
doc.add_paragraph(f'NgÃ y táº¡o bÃ¡o cÃ¡o: {time.strftime("%Y-%m-%d %H:%M:%S")}')
doc.add_paragraph('')

# Má»¥c tiÃªu
doc.add_heading('ğŸ¯ Má»¥c TiÃªu', 2)
doc.add_paragraph('Chuáº©n bá»‹ vÃ  tiá»n xá»­ lÃ½ dá»¯ liá»‡u vÄƒn báº£n tiáº¿ng Viá»‡t cho cÃ¡c mÃ´ hÃ¬nh há»c mÃ¡y vÃ  há»c sÃ¢u.')

# Tá»•ng quan dá»¯ liá»‡u
doc.add_heading('ğŸ“ˆ Tá»•ng Quan Dá»¯ Liá»‡u', 2)

table = doc.add_table(rows=1, cols=2)
table.style = 'Table Grid'
hdr_cells = table.rows[0].cells
hdr_cells[0].text = 'Thá»‘ng KÃª'
hdr_cells[1].text = 'GiÃ¡ Trá»‹'

rows_data = [
    ('Tá»•ng sá»‘ máº«u training', str(len(train_data))),
    ('Tá»•ng sá»‘ máº«u test', str(len(test_data))),
    ('Sá»‘ lá»›p bá»‡nh táº­t', str(len(np.unique(y_train)))),
    ('Sá»‘ tá»« trung bÃ¬nh/cÃ¢u', f"{len(train_data['Question'].str.split().sum())/len(train_data):.1f}")
]

for row_data in rows_data:
    row_cells = table.add_row().cells
    row_cells[0].text = row_data[0]
    row_cells[1].text = row_data[1]

# PhÃ¢n tÃ­ch vÄƒn báº£n gá»‘c
doc.add_heading('ğŸ”¤ PhÃ¢n TÃ­ch VÄƒn Báº£n Gá»‘c', 2)

questions = train_data['Question']
word_counts = questions.str.split().str.len()
char_counts = questions.str.len()

table = doc.add_table(rows=1, cols=2)
table.style = 'Table Grid'
hdr_cells = table.rows[0].cells
hdr_cells[0].text = 'Thá»‘ng KÃª'
hdr_cells[1].text = 'GiÃ¡ Trá»‹'

text_stats = [
    ('Tá»•ng sá»‘ cÃ¢u há»i', str(len(questions))),
    ('Sá»‘ tá»« trung bÃ¬nh/cÃ¢u', f"{word_counts.mean():.1f}"),
    ('Sá»‘ tá»« tá»‘i Ä‘a/cÃ¢u', str(word_counts.max())),
    ('Sá»‘ tá»« tá»‘i thiá»ƒu/cÃ¢u', str(word_counts.min())),
    ('Sá»‘ kÃ½ tá»± trung bÃ¬nh/cÃ¢u', f"{char_counts.mean():.1f}")
]

for stat in text_stats:
    row_cells = table.add_row().cells
    row_cells[0].text = stat[0]
    row_cells[1].text = stat[1]

# Tiá»n xá»­ lÃ½ vÄƒn báº£n
doc.add_heading('ğŸ§¹ Tiá»n Xá»­ LÃ½ VÄƒn Báº£n', 2)
doc.add_paragraph('CÃ¡c bÆ°á»›c xá»­ lÃ½:')
doc.add_paragraph('â€¢ Chuyá»ƒn vá» chá»¯ thÆ°á»ng', style='List Bullet')
doc.add_paragraph('â€¢ Tokenization báº±ng Underthesea', style='List Bullet')
doc.add_paragraph('â€¢ Loáº¡i bá» stopwords cÆ¡ báº£n', style='List Bullet')
doc.add_paragraph('â€¢ Giá»¯ láº¡i chá»‰ tá»« alphabetic', style='List Bullet')

# VÃ­ dá»¥ vÄƒn báº£n Ä‘Ã£ xá»­ lÃ½
doc.add_heading('ğŸ“ VÃ­ Dá»¥ VÄƒn Báº£n ÄÃ£ Xá»­ LÃ½', 2)

table = doc.add_table(rows=1, cols=2)
table.style = 'Table Grid'
hdr_cells = table.rows[0].cells
hdr_cells[0].text = 'VÄƒn Báº£n Gá»‘c'
hdr_cells[1].text = 'VÄƒn Báº£n ÄÃ£ Xá»­ LÃ½'

for i in range(min(5, len(train_data))):
    row_cells = table.add_row().cells
    original = train_data['Question'].iloc[i][:80] + "..." if len(train_data['Question'].iloc[i]) > 80 else train_data['Question'].iloc[i]
    processed = str(train_data['Processed_Question'].iloc[i])[:80] + "..." if len(str(train_data['Processed_Question'].iloc[i])) > 80 else str(train_data['Processed_Question'].iloc[i])
    row_cells[0].text = original
    row_cells[1].text = processed

# Chuáº©n bá»‹ dá»¯ liá»‡u cho cÃ¡c mÃ´ hÃ¬nh
doc.add_heading('ğŸ¤– Chuáº©n Bá»‹ Dá»¯ Liá»‡u Cho CÃ¡c MÃ´ HÃ¬nh', 2)

# Traditional ML
doc.add_heading('ğŸ“Š Dá»¯ Liá»‡u Cho MÃ´ HÃ¬nh Há»c MÃ¡y Truyá»n Thá»‘ng', 3)

table = doc.add_table(rows=1, cols=2)
table.style = 'Table Grid'
hdr_cells = table.rows[0].cells
hdr_cells[0].text = 'ThÃ´ng Sá»‘'
hdr_cells[1].text = 'GiÃ¡ Trá»‹'

ml_stats = [
    ('KÃ­ch thÆ°á»›c tá»« vá»±ng', str(X_train_ml.shape[1])),
    ('Sá»‘ máº«u training', str(X_train_ml.shape[0])),
    ('Sá»‘ tÃ­nh nÄƒng', str(X_train_ml.shape[1])),
    ('Tá»· lá»‡ sparsity', f"{np.sum(X_train_ml) / X_train_ml.size:.3f}")
]

for stat in ml_stats:
    row_cells = table.add_row().cells
    row_cells[0].text = stat[0]
    row_cells[1].text = stat[1]

# Deep Learning
doc.add_heading('ğŸ§  Dá»¯ Liá»‡u Cho MÃ´ HÃ¬nh Há»c SÃ¢u', 3)

vocab_size = dl_metadata['vocab_size']
max_seq_len = dl_metadata['max_sequence_length']

table = doc.add_table(rows=1, cols=2)
table.style = 'Table Grid'
hdr_cells = table.rows[0].cells
hdr_cells[0].text = 'ThÃ´ng Sá»‘'
hdr_cells[1].text = 'GiÃ¡ Trá»‹'

dl_stats = [
    ('KÃ­ch thÆ°á»›c tá»« vá»±ng', str(vocab_size)),
    ('Sá»‘ máº«u training', str(X_train_dl.shape[0])),
    ('Äá»™ dÃ i sequence tá»‘i Ä‘a', str(max_seq_len)),
    ('Tá»•ng sá»‘ tá»« duy nháº¥t', str(len(tokenizer.word_index)))
]

for stat in dl_stats:
    row_cells = table.add_row().cells
    row_cells[0].text = stat[0]
    row_cells[1].text = stat[1]

# VÃ­ dá»¥ tokenization
doc.add_heading('ğŸ”¢ VÃ­ Dá»¥ Tokenization', 3)

sample_text = train_data['Processed_Question'].iloc[0]
sample_sequence = X_train_dl[0][:20]

p = doc.add_paragraph()
p.add_run('VÄƒn báº£n gá»‘c: ').bold = True
p.add_run(sample_text)

p = doc.add_paragraph()
p.add_run('Sequence (20 token Ä‘áº§u): ').bold = True
p.add_run(str(sample_sequence.tolist()))

p = doc.add_paragraph()
p.add_run('Mapping má»™t sá»‘ token:').bold = True

word_index = tokenizer.word_index
for word, idx in list(word_index.items())[:10]:
    doc.add_paragraph(f'{word} â†’ {idx}', style='List Bullet')

# PhÃ¢n tÃ­ch nhÃ£n
doc.add_heading('ğŸ·ï¸ PhÃ¢n TÃ­ch NhÃ£n (Labels)', 2)

table = doc.add_table(rows=1, cols=2)
table.style = 'Table Grid'
hdr_cells = table.rows[0].cells
hdr_cells[0].text = 'ThÃ´ng Sá»‘'
hdr_cells[1].text = 'GiÃ¡ Trá»‹'

label_stats = [
    ('Tá»•ng sá»‘ lá»›p bá»‡nh', str(len(label_encoder.classes_))),
    ('Lá»›p cÃ³ nhiá»u máº«u nháº¥t', str(max(Counter(y_train).values()))),
    ('Lá»›p cÃ³ Ã­t máº«u nháº¥t', str(min(Counter(y_train).values()))),
    ('ID lá»›p phá»• biáº¿n nháº¥t', str(Counter(y_train).most_common(1)[0][0]))
]

for stat in label_stats:
    row_cells = table.add_row().cells
    row_cells[0].text = stat[0]
    row_cells[1].text = stat[1]

# Top 10 lá»›p bá»‡nh
doc.add_heading('ğŸ“‹ Top 10 Lá»›p Bá»‡nh Phá»• Biáº¿n Nháº¥t', 3)

table = doc.add_table(rows=1, cols=3)
table.style = 'Table Grid'
hdr_cells = table.rows[0].cells
hdr_cells[0].text = 'TÃªn Bá»‡nh'
hdr_cells[1].text = 'Sá»‘ Máº«u'
hdr_cells[2].text = 'Tá»· Lá»‡ (%)'

label_counts = Counter(y_train)
total_samples = len(y_train)
for label_id, count in label_counts.most_common(10):
    disease_name = label_encoder.inverse_transform([label_id])[0]
    percentage = (count / total_samples) * 100
    row_cells = table.add_row().cells
    row_cells[0].text = disease_name
    row_cells[1].text = str(count)
    row_cells[2].text = f"{percentage:.2f}%"

# Tá»‡p Ä‘Ã£ lÆ°u
doc.add_heading('ğŸ’¾ Tá»‡p ÄÃ£ LÆ°u', 2)
files_list = [
    'data/X_train.npy - Features training (Traditional ML)',
    'data/y_train.npy - Labels training',
    'data/X_test.npy - Features test (Traditional ML)',
    'data/y_test.npy - Labels test',
    'data/X_train_dl.npy - Sequences training (Deep Learning)',
    'data/X_test_dl.npy - Sequences test (Deep Learning)',
    'data/tokenizer.joblib - Tokenizer cho Deep Learning',
    'data/label_encoder.joblib - Label Encoder',
    'data/dl_metadata.joblib - Metadata cho Deep Learning'
]

for file_desc in files_list:
    doc.add_paragraph(file_desc, style='List Bullet')

# Tá»•ng káº¿t
doc.add_heading('ğŸ“ Tá»•ng Káº¿t', 2)
summary_text = f"""ÄÃ£ hoÃ n thÃ nh tiá»n xá»­ lÃ½ dá»¯ liá»‡u:
â€¢ Xá»­ lÃ½ {len(train_data)} máº«u training vÃ  {len(test_data)} máº«u test
â€¢ Táº¡o dá»¯ liá»‡u cho {len(np.unique(y_train))} loáº¡i bá»‡nh khÃ¡c nhau
â€¢ Chuáº©n bá»‹ dá»¯ liá»‡u cho cáº£ mÃ´ hÃ¬nh há»c mÃ¡y truyá»n thá»‘ng vÃ  há»c sÃ¢u
â€¢ Tá»« vá»±ng: {vocab_size} tá»« cho Deep Learning, {X_train_ml.shape[1]} tÃ­nh nÄƒng cho Traditional ML
â€¢ Sáºµn sÃ ng cho bÆ°á»›c tiáº¿p theo: Lá»±a chá»n vÃ  huáº¥n luyá»‡n mÃ´ hÃ¬nh"""

doc.add_paragraph(summary_text)

# LÆ°u file
doc.save('reports/Step2_Data_Preprocessing.docx')
print("âœ… ÄÃ£ táº¡o bÃ¡o cÃ¡o DOCX: reports/Step2_Data_Preprocessing.docx")